{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS4610 Introduction to Data Analytics || Course Project \n",
    "### Data Cleaning and Augmentation\n",
    "Notebook by **Group 12**\n",
    "\n",
    "This notebook undertakes operations like correcting data types, names given to missing values, etc. Also, data columns have been (externally) given more understandable names to ease referencing (please check out the EDA notebook for information on column names). The following operations have been performed:\n",
    "\n",
    "1. Missing value tags (missing, na, N/A) replaced with `numpy.nan`\n",
    "2. Label encoding some categorical columns and typecasting to appropriate dtypes \n",
    "\n",
    "**NOTE**: Synthetic imputation of missing data has been performed externally through normal Python scripts. Since these processes are computationally very expensive and time consuming, they were run on **Google Colab** kernels with GPU support. The code files are available in the main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "# Data handling and transformation libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imputation libraries\n",
    "\n",
    "from impyute.imputation.cs import mice\n",
    "from impyute.imputation.cs import fast_knn\n",
    "from missingpy import MissForest\n",
    "\n",
    "# Resampling library\n",
    "\n",
    "from imblearn.over_sampling import SMOTE as smote\n",
    "\n",
    "# Other libraries\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.setrecursionlimit(1000000)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "\n",
    "train = pd.read_csv('.././data/train.csv')\n",
    "train_mf = pd.read_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf.csv\")\n",
    "train_mf_res = pd.read_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83000 entries, 0 to 82999\n",
      "Data columns (total 50 columns):\n",
      "application_key          83000 non-null int64\n",
      "credit_score             83000 non-null object\n",
      "risk_score               77114 non-null float64\n",
      "sev_def_any              82465 non-null float64\n",
      "sev_def_auto             82465 non-null float64\n",
      "sev_def_edu              82465 non-null float64\n",
      "min_credit_rev           83000 non-null object\n",
      "max_credit_act           83000 non-null object\n",
      "max_credit_act_rev       83000 non-null object\n",
      "total_credit_1_miss      83000 non-null object\n",
      "total_credit             83000 non-null object\n",
      "due_collected            83000 non-null object\n",
      "total_due                83000 non-null object\n",
      "annual_pay               83000 non-null object\n",
      "annual_income            83000 non-null int64\n",
      "property_value           83000 non-null object\n",
      "fc_cards_act_rev         83000 non-null object\n",
      "fc_cards_act             83000 non-null object\n",
      "fc_lines_act             83000 non-null object\n",
      "pc_cards_act             83000 non-null object\n",
      "pc_lines_act             83000 non-null object\n",
      "loan_util_act_rev        59538 non-null float64\n",
      "line_util_past2          52332 non-null float64\n",
      "line_util_past1          40689 non-null float64\n",
      "line_util_1_miss         63470 non-null float64\n",
      "tenure_act_rev           83000 non-null object\n",
      "tenure_oldest_act        83000 non-null object\n",
      "tenure_oldest_act_rev    83000 non-null object\n",
      "last_miss_time           83000 non-null object\n",
      "tenure_oldest_line       83000 non-null object\n",
      "max_tenure_auto          83000 non-null object\n",
      "max_tenure_edu           83000 non-null object\n",
      "total_tenure_act         83000 non-null object\n",
      "residence_time           81131 non-null float64\n",
      "lines_act_1_miss         83000 non-null object\n",
      "cards_rev_1_miss         83000 non-null object\n",
      "lines_act                83000 non-null object\n",
      "cards_act_t2             83000 non-null object\n",
      "lines_act_2yrs           83000 non-null object\n",
      "lines_deli               83000 non-null object\n",
      "line_util_edu            83000 non-null object\n",
      "line_util_auto           83000 non-null object\n",
      "stress_index             83000 non-null object\n",
      "lines_high_risk          83000 non-null object\n",
      "max_due_ratio            74851 non-null float64\n",
      "mort_2_miss              83000 non-null object\n",
      "auto_2_miss              83000 non-null object\n",
      "card_type                83000 non-null object\n",
      "location_id              83000 non-null int64\n",
      "default_ind              83000 non-null int64\n",
      "dtypes: float64(10), int64(4), object(36)\n",
      "memory usage: 31.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions\n",
    "In this section, we have defined some functions that are commonly used in data cleaning. This makes readability of code and repeatability of operations convenient down the line. Available functions:\n",
    "1. **missing_table**: Tally of missing values in the dataset by column, arranged in descending order.\n",
    "2. **drop_bad_rows**: Drops rows with more percentage of null values than specified threshold\n",
    "3. **drop_bad_cols**: Drops columns with more percentage of null values than specified threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_table(df, threshold=None, ascending=False):\n",
    "    \"\"\"\n",
    "    Counts number of missing values and percentage of missing values in every column\n",
    "    of input pandas DataFrame object.\n",
    "    \n",
    "    :params: \n",
    "        threshold: (int/float) returns only those columns with number/percentage of missing\n",
    "                   values higher than this value, default is None (returns all columns)\n",
    "        ascending: (boolean) sorts table in ascending order of missing values\n",
    "                   if set to True, default is False\n",
    "                   \n",
    "    :return: columns with missing values above threshold; pandas DataFrame object with column \n",
    "             name, number of missing values and percentage of missing values\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    miss_vals = np.array([df[col].isnull().sum() for col in cols])\n",
    "    miss_vals_percent = (miss_vals / len(df))*100\n",
    "    \n",
    "    miss_table = pd.DataFrame(np.vstack((cols, miss_vals, miss_vals_percent)).T,\n",
    "                              columns=['column', 'missing values', '% missing values'])\n",
    "    \n",
    "    if threshold is None:\n",
    "        miss_table = miss_table.sort_values(by='missing values', ascending=ascending)\n",
    "        return miss_table\n",
    "    else:\n",
    "        if threshold <= 1.0:\n",
    "            ret = miss_table.loc[miss_table['% missing values'] >= threshold*100.0, :]\n",
    "            return ret.sort_values(by='missing values', ascending=ascending)\n",
    "        elif threshold > 1.0:\n",
    "            ret = miss_table.loc[miss_table['missing values'] >= threshold, :]\n",
    "            return ret.sort_values(by='missing values', ascending=ascending)\n",
    "        else:\n",
    "            raise ValueError('Invalid threshold type')\n",
    "            \n",
    "            \n",
    "def drop_bad_rows(df, target=None, threshold=0.8):\n",
    "    \n",
    "    bad_rows = train.loc[train.isnull().sum(axis=1)/train.shape[1]>=threshold, :]\n",
    "    if target is not None:\n",
    "        class0, class1 = bad_rows[target].value_counts().values[0], bad_rows[target].value_counts().values[1]\n",
    "        class0_pc, class1_pc = class0/len(df), class1/len(df)\n",
    "        print(\"Dropped %d (%.4f%%) class 0 and %d (%.4f%%) class 1 examples\" % \n",
    "              (class0, class0_pc*100, class1, class1_pc*100))\n",
    "    df_new = df.drop(bad_rows.index, axis=0)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def drop_bad_cols(df, threshold=0.4):\n",
    "    \n",
    "    miss_table = missing_table(df, threshold=threshold)\n",
    "    bad_cols = miss_table['column'].values.tolist()\n",
    "    df_new = df.drop(bad_cols, axis=1)\n",
    "    print(\"Dropped %d of %d columns\" % (len(bad_cols), df.shape[1]))\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values tag correction\n",
    "Missing values have been represented with variety of strings. It is difficult to deal with them during data exploration with `pandas`. We will convert all of them into numpy not-a-number values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "# Looking for a vectorized implementation of this operation\n",
    "# The code below takes about 10 seconds to run, which is quite slow\n",
    "\n",
    "miss_tags = ['missing', 'na', 'N/A']\n",
    "\n",
    "for col in train.columns:\n",
    "    for i in range(len(train)):\n",
    "        if train.at[i, col] in miss_tags:\n",
    "            train.at[i, col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation now gives a clearer picture of the dataset when accessed using `info()` attribute of the DataFrame. Very few columns have all non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83000 entries, 0 to 82999\n",
      "Data columns (total 50 columns):\n",
      "application_key          83000 non-null int64\n",
      "credit_score             79267 non-null object\n",
      "risk_score               77114 non-null float64\n",
      "sev_def_any              82465 non-null float64\n",
      "sev_def_auto             82465 non-null float64\n",
      "sev_def_edu              82465 non-null float64\n",
      "min_credit_rev           63299 non-null object\n",
      "max_credit_act           75326 non-null object\n",
      "max_credit_act_rev       63291 non-null object\n",
      "total_credit_1_miss      71318 non-null object\n",
      "total_credit             82465 non-null object\n",
      "due_collected            36283 non-null object\n",
      "total_due                68422 non-null object\n",
      "annual_pay               73311 non-null object\n",
      "annual_income            83000 non-null int64\n",
      "property_value           49481 non-null object\n",
      "fc_cards_act_rev         63757 non-null object\n",
      "fc_cards_act             66501 non-null object\n",
      "fc_lines_act             67641 non-null object\n",
      "pc_cards_act             82995 non-null object\n",
      "pc_lines_act             82465 non-null object\n",
      "loan_util_act_rev        59538 non-null float64\n",
      "line_util_past2          52332 non-null float64\n",
      "line_util_past1          40689 non-null float64\n",
      "line_util_1_miss         63470 non-null float64\n",
      "tenure_act_rev           75138 non-null object\n",
      "tenure_oldest_act        72071 non-null object\n",
      "tenure_oldest_act_rev    69350 non-null object\n",
      "last_miss_time           82465 non-null object\n",
      "tenure_oldest_line       82465 non-null object\n",
      "max_tenure_auto          45012 non-null object\n",
      "max_tenure_edu           24461 non-null object\n",
      "total_tenure_act         75138 non-null object\n",
      "residence_time           81131 non-null float64\n",
      "lines_act_1_miss         82465 non-null object\n",
      "cards_rev_1_miss         48132 non-null object\n",
      "lines_act                79841 non-null object\n",
      "cards_act_t2             75138 non-null object\n",
      "lines_act_2yrs           82465 non-null object\n",
      "lines_deli               76671 non-null object\n",
      "line_util_edu            17930 non-null object\n",
      "line_util_auto           25736 non-null object\n",
      "stress_index             80977 non-null object\n",
      "lines_high_risk          82111 non-null object\n",
      "max_due_ratio            74851 non-null float64\n",
      "mort_2_miss              37080 non-null object\n",
      "auto_2_miss              59397 non-null object\n",
      "card_type                83000 non-null object\n",
      "location_id              83000 non-null int64\n",
      "default_ind              83000 non-null int64\n",
      "dtypes: float64(10), int64(4), object(36)\n",
      "memory usage: 31.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# DataFrame information post null value tagging\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding and Typecasting\n",
    "Some columns have type `object` and need encoding into `float` types so that further processing on them be possible. In this section, we perform relevant transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding categorical columns\n",
    "\n",
    "enc_columns = ['card_type']\n",
    "\n",
    "le = LabelEncoder()\n",
    "train[enc_columns] = le.fit_transform(train[enc_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all columns to float data type\n",
    "# We only have two categorical columns (card_type and location_id) which we can \n",
    "# change back to categorical type later\n",
    "\n",
    "for col in train.columns:\n",
    "    train[col] = train[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 378 (0.4554%) class 0 and 157 (0.1892%) class 1 examples\n",
      "Dropped 9 of 50 columns\n"
     ]
    }
   ],
   "source": [
    "# First, we will remove some rows and columns with too many missing values\n",
    "\n",
    "train = drop_bad_rows(train, target='default_ind', threshold=0.75)\n",
    "train = drop_bad_cols(train, threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n",
    "In this section, we will try some methods for synthetic imputation of missing values in the dataset. Based on model performance on these imputed datasets, we will choose the best imputation method. The following methods have been used.\n",
    "1. **Zero imputation**: All missing values replaced with zero\n",
    "2. **Missing Forest imputation**: A random forest imputes each column assuming it's the target and the other columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero imputation\n",
    "\n",
    "train_zero = train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Forest imputation\n",
    "# NOTE (DO NOT RUN ON JUPYTER):\n",
    "# This imputation is very computationally expensive and time consuming\n",
    "# Was performed externally on PyCharm\n",
    "\n",
    "cols = train.columns.tolist()\n",
    "\n",
    "# Impute values\n",
    "# Function returns a numpy ndarray, which we convert to DataFrame again\n",
    "imputer = MissForest()\n",
    "\n",
    "print(\"[INFO] Imputation started\")\n",
    "X_imputed = imputer.fit_transform(train.values)\n",
    "\n",
    "print(\"[INFO] Imputation complete\")\n",
    "train_mf = pd.DataFrame(X_imputed, columns=cols)\n",
    "\n",
    "# Save new DataFrame to drive\n",
    "train_mf.to_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling minority class using SMOTE\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** is a novel method for reducing/removing class imbalance in datasets. In the cell below, we perform SMOTE resampling on Missing Forest imputed data to obtained a resampled dataset with equal representation of defaulters and non-defaulters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increase in dataset size: 42.53% (35069 rows added)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "cols = train_mf.columns.tolist()\n",
    "y = train_mf.default_ind.values\n",
    "X = train_mf.drop(['default_ind'], axis=1).values\n",
    "\n",
    "# SMOTE resampling\n",
    "sm = smote(sampling_strategy='auto', random_state=123)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "pc_increase = (X_res.shape[0] - X.shape[0])*100/X.shape[0]\n",
    "print(\"Increase in dataset size: %.2f%% (%d rows added)\" % (pc_increase, X_res.shape[0] - X.shape[0]))\n",
    "\n",
    "# Reform dataframe\n",
    "res_df_vals = np.hstack((X_res, y_res.reshape((-1, 1))))\n",
    "res_df = pd.DataFrame(res_df_vals, columns=cols)\n",
    "\n",
    "res_df = res_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "res_df.to_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something interesting happened\n",
    "To make sure nothing's wrong with the way our newest model is working, I want to split our data manually into training and test, to see a final validation of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_mf_res\n",
    "col_names = data.columns.tolist()\n",
    "\n",
    "y = data.default_ind.values\n",
    "X = data.drop('default_ind', axis=1).values\n",
    "\n",
    "# Split dataset 80-20 into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=123)\n",
    "\n",
    "# Recombine train and test X and y\n",
    "train = np.hstack((X_train, y_train.reshape((-1, 1))))\n",
    "test = np.hstack((X_test, y_test.reshape(-1, 1)))\n",
    "\n",
    "# Convert to dataframes and save to drive\n",
    "train_final = pd.DataFrame(train, columns=col_names)\n",
    "test_final = pd.DataFrame(test, columns=col_names)\n",
    "\n",
    "train_final.to_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_final.csv\", index=False)\n",
    "test_final.to_csv(\"/home/nishant/Desktop/IDA Project/mod_data/test_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
