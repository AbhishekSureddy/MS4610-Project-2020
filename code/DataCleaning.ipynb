{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS4610 Introduction to Data Analytics || Course Project \n",
    "### Data Cleaning and Augmentation\n",
    "Notebook by **Group 12**\n",
    "\n",
    "This notebook undertakes operations like correcting data types, names given to missing values, etc. Also, data columns have been (externally) given more understandable names to ease referencing (please check out the EDA notebook for information on column names). The following operations have been performed:\n",
    "\n",
    "1. Missing value tags (missing, na, N/A) replaced with `numpy.nan`\n",
    "2. Label encoding some categorical columns and typecasting to appropriate dtypes \n",
    "\n",
    "**NOTE**: Synthetic imputation of missing data has been performed externally through normal Python scripts. Since these processes are computationally very expensive and time consuming, they were run on **Google Colab** kernels with GPU support. The code files are available in the main repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "# Data handling and transformation libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imputation libraries\n",
    "\n",
    "from impyute.imputation.cs import mice\n",
    "from impyute.imputation.cs import fast_knn\n",
    "from missingpy import MissForest\n",
    "\n",
    "# Resampling library\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE as smote\n",
    "\n",
    "# Other libraries\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.setrecursionlimit(1000000)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "\n",
    "train = pd.read_csv('.././data/train.csv')\n",
    "train_mf = pd.read_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf.csv\")\n",
    "train_mf_res = pd.read_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83000 entries, 0 to 82999\n",
      "Data columns (total 50 columns):\n",
      "application_key          83000 non-null int64\n",
      "credit_score             83000 non-null object\n",
      "risk_score               77114 non-null float64\n",
      "sev_def_any              82465 non-null float64\n",
      "sev_def_auto             82465 non-null float64\n",
      "sev_def_edu              82465 non-null float64\n",
      "min_credit_rev           83000 non-null object\n",
      "max_credit_act           83000 non-null object\n",
      "max_credit_act_rev       83000 non-null object\n",
      "total_credit_1_miss      83000 non-null object\n",
      "total_credit             83000 non-null object\n",
      "due_collected            83000 non-null object\n",
      "total_due                83000 non-null object\n",
      "annual_pay               83000 non-null object\n",
      "annual_income            83000 non-null int64\n",
      "property_value           83000 non-null object\n",
      "fc_cards_act_rev         83000 non-null object\n",
      "fc_cards_act             83000 non-null object\n",
      "fc_lines_act             83000 non-null object\n",
      "pc_cards_act             83000 non-null object\n",
      "pc_lines_act             83000 non-null object\n",
      "loan_util_act_rev        59538 non-null float64\n",
      "line_util_past2          52332 non-null float64\n",
      "line_util_past1          40689 non-null float64\n",
      "line_util_1_miss         63470 non-null float64\n",
      "tenure_act_rev           83000 non-null object\n",
      "tenure_oldest_act        83000 non-null object\n",
      "tenure_oldest_act_rev    83000 non-null object\n",
      "last_miss_time           83000 non-null object\n",
      "tenure_oldest_line       83000 non-null object\n",
      "max_tenure_auto          83000 non-null object\n",
      "max_tenure_edu           83000 non-null object\n",
      "total_tenure_act         83000 non-null object\n",
      "residence_time           81131 non-null float64\n",
      "lines_act_1_miss         83000 non-null object\n",
      "cards_rev_1_miss         83000 non-null object\n",
      "lines_act                83000 non-null object\n",
      "cards_act_t2             83000 non-null object\n",
      "lines_act_2yrs           83000 non-null object\n",
      "lines_deli               83000 non-null object\n",
      "line_util_edu            83000 non-null object\n",
      "line_util_auto           83000 non-null object\n",
      "stress_index             83000 non-null object\n",
      "lines_high_risk          83000 non-null object\n",
      "max_due_ratio            74851 non-null float64\n",
      "mort_2_miss              83000 non-null object\n",
      "auto_2_miss              83000 non-null object\n",
      "card_type                83000 non-null object\n",
      "location_id              83000 non-null int64\n",
      "default_ind              83000 non-null int64\n",
      "dtypes: float64(10), int64(4), object(36)\n",
      "memory usage: 31.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions\n",
    "In this section, we have defined some functions that are commonly used in data cleaning. This makes readability of code and repeatability of operations convenient down the line. Available functions:\n",
    "1. **missing_table**: Tally of missing values in the dataset by column, arranged in descending order.\n",
    "2. **drop_bad_rows**: Drops rows with more percentage of null values than specified threshold\n",
    "3. **drop_bad_cols**: Drops columns with more percentage of null values than specified threshold\n",
    "4. **create_embedding**: Generates embeddings for categorical features when provided suitable arguments. See Model Selection notebook for model details about categorical feature embeddings.\n",
    "5. **scale_df**: Standard scaling function for passed columns of a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_table(df, threshold=None, ascending=False):\n",
    "    \"\"\"\n",
    "    Counts number of missing values and percentage of missing values in every column\n",
    "    of input pandas DataFrame object.\n",
    "    \n",
    "    :params: \n",
    "        threshold: (int/float) returns only those columns with number/percentage of missing\n",
    "                   values higher than this value, default is None (returns all columns)\n",
    "        ascending: (boolean) sorts table in ascending order of missing values\n",
    "                   if set to True, default is False\n",
    "                   \n",
    "    :return: columns with missing values above threshold; pandas DataFrame object with column \n",
    "             name, number of missing values and percentage of missing values\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    miss_vals = np.array([df[col].isnull().sum() for col in cols])\n",
    "    miss_vals_percent = (miss_vals / len(df))*100\n",
    "    \n",
    "    miss_table = pd.DataFrame(np.vstack((cols, miss_vals, miss_vals_percent)).T,\n",
    "                              columns=['column', 'missing values', '% missing values'])\n",
    "    \n",
    "    if threshold is None:\n",
    "        miss_table = miss_table.sort_values(by='missing values', ascending=ascending)\n",
    "        return miss_table\n",
    "    else:\n",
    "        if threshold <= 1.0:\n",
    "            ret = miss_table.loc[miss_table['% missing values'] >= threshold*100.0, :]\n",
    "            return ret.sort_values(by='missing values', ascending=ascending)\n",
    "        elif threshold > 1.0:\n",
    "            ret = miss_table.loc[miss_table['missing values'] >= threshold, :]\n",
    "            return ret.sort_values(by='missing values', ascending=ascending)\n",
    "        else:\n",
    "            raise ValueError('Invalid threshold type')\n",
    "            \n",
    "            \n",
    "def drop_bad_rows(df, target=None, threshold=0.8):\n",
    "    \n",
    "    bad_rows = train.loc[train.isnull().sum(axis=1)/train.shape[1]>=threshold, :]\n",
    "    if target is not None:\n",
    "        class0, class1 = bad_rows[target].value_counts().values[0], bad_rows[target].value_counts().values[1]\n",
    "        class0_pc, class1_pc = class0/len(df), class1/len(df)\n",
    "        print(\"Dropped %d (%.4f%%) class 0 and %d (%.4f%%) class 1 examples\" % \n",
    "              (class0, class0_pc*100, class1, class1_pc*100))\n",
    "    df_new = df.drop(bad_rows.index, axis=0)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def drop_bad_cols(df, threshold=0.4):\n",
    "    \n",
    "    miss_table = missing_table(df, threshold=threshold)\n",
    "    bad_cols = miss_table['column'].values.tolist()\n",
    "    df_new = df.drop(bad_cols, axis=1)\n",
    "    print(\"Dropped %d of %d columns\" % (len(bad_cols), df.shape[1]))\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def create_embedding(train_mf, test_mf, col, batch_size=100):\n",
    "    \n",
    "    # Dependencies\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    \n",
    "    # EMBEDDING ==========================================================================================\n",
    "\n",
    "    labels = train_mf.default_ind.values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train_mf[col] = le.fit_transform(train_mf[col])\n",
    "    test_mf[col] = le.transform(test_mf[col])\n",
    "\n",
    "    vocab_size = train_mf[col].nunique()\n",
    "    max_length = 1\n",
    "    embed_vec_size = min(50, (train_mf[col].nunique()+1)//2)\n",
    "    col_train = train_mf[col].values\n",
    "    col_test = test_mf[col].values\n",
    "\n",
    "    # Define model\n",
    "    print('[INFO] Generating embeddings')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_vec_size, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    model.fit(col_train, labels, epochs=1, batch_size=batch_size)\n",
    "\n",
    "    # Get outputs of first layer\n",
    "    new_model = Sequential()\n",
    "    new_model.add(Embedding(vocab_size, embed_vec_size, input_length=max_length))\n",
    "\n",
    "    new_model.set_weights(model.layers[0].get_weights())\n",
    "    new_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    preds_train = new_model.predict(col_train).reshape(-1, embed_vec_size)\n",
    "    preds_test = new_model.predict(col_test).reshape(-1, embed_vec_size)\n",
    "    embed_cols_train = [str(col)+'_emb'+str(i) for i in range(preds_train.shape[1])]\n",
    "    embed_cols_test = [str(col)+'_emb'+str(i) for i in range(preds_test.shape[1])]\n",
    "    col_embed_df_train = pd.DataFrame(preds_train, columns=embed_cols_train)\n",
    "    col_embed_df_test = pd.DataFrame(preds_test, columns=embed_cols_test)\n",
    "\n",
    "    return col_embed_df_train, col_embed_df_test\n",
    "\n",
    "\n",
    "def scale_df(train_mf, test_mf, non_scale_cols, replace_cols):\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    X_sc_train = ss.fit_transform(train_mf.drop(non_scale_cols, axis=1).values)\n",
    "    X_sc_test = ss.transform(test_mf.drop(non_scale_cols, axis=1).values)\n",
    "\n",
    "    X_df_train = pd.DataFrame(X_sc_train)\n",
    "    X_df_train[replace_cols] = train_mf[replace_cols].values\n",
    "\n",
    "    X_df_test = pd.DataFrame(X_sc_test)\n",
    "    X_df_test[replace_cols] = test_mf[replace_cols].values\n",
    "    \n",
    "    return X_df_train, X_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values tag correction\n",
    "Missing values have been represented with variety of strings. It is difficult to deal with them during data exploration with `pandas`. We will convert all of them into numpy not-a-number values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "# Looking for a vectorized implementation of this operation\n",
    "# The code below takes about 10 seconds to run, which is quite slow\n",
    "\n",
    "miss_tags = ['missing', 'na', 'N/A']\n",
    "\n",
    "for col in train.columns:\n",
    "    for i in range(len(train)):\n",
    "        if train.at[i, col] in miss_tags:\n",
    "            train.at[i, col] = np.nan\n",
    "\n",
    "# train = train.replace(miss_tags, [np.nan, np.nan, np.nan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation now gives a clearer picture of the dataset when accessed using `info()` attribute of the DataFrame. Very few columns have all non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83000 entries, 0 to 82999\n",
      "Data columns (total 50 columns):\n",
      "application_key          83000 non-null int64\n",
      "credit_score             79267 non-null object\n",
      "risk_score               77114 non-null float64\n",
      "sev_def_any              82465 non-null float64\n",
      "sev_def_auto             82465 non-null float64\n",
      "sev_def_edu              82465 non-null float64\n",
      "min_credit_rev           63299 non-null object\n",
      "max_credit_act           75326 non-null object\n",
      "max_credit_act_rev       63291 non-null object\n",
      "total_credit_1_miss      71318 non-null object\n",
      "total_credit             82465 non-null object\n",
      "due_collected            36283 non-null object\n",
      "total_due                68422 non-null object\n",
      "annual_pay               73311 non-null object\n",
      "annual_income            83000 non-null int64\n",
      "property_value           49481 non-null object\n",
      "fc_cards_act_rev         63757 non-null object\n",
      "fc_cards_act             66501 non-null object\n",
      "fc_lines_act             67641 non-null object\n",
      "pc_cards_act             82995 non-null object\n",
      "pc_lines_act             82465 non-null object\n",
      "loan_util_act_rev        59538 non-null float64\n",
      "line_util_past2          52332 non-null float64\n",
      "line_util_past1          40689 non-null float64\n",
      "line_util_1_miss         63470 non-null float64\n",
      "tenure_act_rev           75138 non-null object\n",
      "tenure_oldest_act        72071 non-null object\n",
      "tenure_oldest_act_rev    69350 non-null object\n",
      "last_miss_time           82465 non-null object\n",
      "tenure_oldest_line       82465 non-null object\n",
      "max_tenure_auto          45012 non-null object\n",
      "max_tenure_edu           24461 non-null object\n",
      "total_tenure_act         75138 non-null object\n",
      "residence_time           81131 non-null float64\n",
      "lines_act_1_miss         82465 non-null object\n",
      "cards_rev_1_miss         48132 non-null object\n",
      "lines_act                79841 non-null object\n",
      "cards_act_t2             75138 non-null object\n",
      "lines_act_2yrs           82465 non-null object\n",
      "lines_deli               76671 non-null object\n",
      "line_util_edu            17930 non-null object\n",
      "line_util_auto           25736 non-null object\n",
      "stress_index             80977 non-null object\n",
      "lines_high_risk          82111 non-null object\n",
      "max_due_ratio            74851 non-null float64\n",
      "mort_2_miss              37080 non-null object\n",
      "auto_2_miss              59397 non-null object\n",
      "card_type                83000 non-null object\n",
      "location_id              83000 non-null int64\n",
      "default_ind              83000 non-null int64\n",
      "dtypes: float64(10), int64(4), object(36)\n",
      "memory usage: 31.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# DataFrame information post null value tagging\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding and Typecasting\n",
    "Some columns have type `object` and need encoding into `float` types so that further processing on them be possible. In this section, we perform relevant transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding categorical columns\n",
    "\n",
    "enc_columns = ['card_type']\n",
    "\n",
    "le = LabelEncoder()\n",
    "train[enc_columns] = le.fit_transform(train[enc_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all columns to float data type\n",
    "# We only have two categorical columns (card_type and location_id) which we can \n",
    "# change back to categorical type later\n",
    "\n",
    "for col in train.columns:\n",
    "    train[col] = train[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 378 (0.4554%) class 0 and 157 (0.1892%) class 1 examples\n",
      "Dropped 9 of 50 columns\n"
     ]
    }
   ],
   "source": [
    "# First, we will remove some rows and columns with too many missing values\n",
    "\n",
    "train = drop_bad_rows(train, target='default_ind', threshold=0.75)\n",
    "train = drop_bad_cols(train, threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n",
    "In this section, we will try some methods for synthetic imputation of missing values in the dataset. Based on model performance on these imputed datasets, we will choose the best imputation method. The following methods have been used.\n",
    "1. **Zero imputation**: All missing values replaced with zero\n",
    "2. **Missing Forest imputation**: A random forest imputes each column assuming it's the target and the other columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero imputation\n",
    "\n",
    "train_zero = train.fillna(0)\n",
    "train_zero.to_csv(\".././mod_data/train_zero.csv\", index=False)\n",
    "train.to_csv(\".././mod_data/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Forest imputation\n",
    "# NOTE (DO NOT RUN ON JUPYTER):\n",
    "# This imputation is very computationally expensive and time consuming\n",
    "# Was performed externally on PyCharm\n",
    "\n",
    "cols = train.columns.tolist()\n",
    "\n",
    "# Impute values\n",
    "# Function returns a numpy ndarray, which we convert to DataFrame again\n",
    "imputer = MissForest()\n",
    "\n",
    "print(\"[INFO] Imputation started\")\n",
    "X_imputed = imputer.fit_transform(train.values)\n",
    "\n",
    "print(\"[INFO] Imputation complete\")\n",
    "train_mf = pd.DataFrame(X_imputed, columns=cols)\n",
    "\n",
    "# Save new DataFrame to drive\n",
    "train_mf.to_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling minority class using SMOTE\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** is a novel method for reducing/removing class imbalance in datasets. In the cell below, we perform SMOTE resampling on Missing Forest imputed data to obtained a resampled dataset with equal representation of defaulters and non-defaulters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increase in dataset size: 42.53% (35069 rows added)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "cols = train_mf.columns.tolist()\n",
    "y = train_mf.default_ind.values\n",
    "X = train_mf.drop(['default_ind'], axis=1).values\n",
    "\n",
    "# SMOTE resampling\n",
    "sm = smote(sampling_strategy='auto', random_state=123)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "pc_increase = (X_res.shape[0] - X.shape[0])*100/X.shape[0]\n",
    "print(\"Increase in dataset size: %.2f%% (%d rows added)\" % (pc_increase, X_res.shape[0] - X.shape[0]))\n",
    "\n",
    "# Reform dataframe\n",
    "res_df_vals = np.hstack((X_res, y_res.reshape((-1, 1))))\n",
    "res_df = pd.DataFrame(res_df_vals, columns=cols)\n",
    "\n",
    "res_df = res_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "res_df.to_csv(\"/home/nishant/Desktop/IDA Project/mod_data/train_mf_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning continuous variables\n",
    "In this final attempt, we discretize all continuous variables by making quantile cuts on each continuous column. This will help us obtain as many categorical variables. Using these, we will test the performance of CatBoost and enitity embeddings with neural networks. **Motivation**: Our target is categorical and we are relying mostly on decision tree models. These models find decision rules to make predictions by computing thresholds for minimising entropy after split for continuous data. By reducing the number of unique values, we make the splitting procedure easier and more semantically interpretable. Let's see if it improves model performance also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_key</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>sev_def_any</th>\n",
       "      <th>sev_def_auto</th>\n",
       "      <th>sev_def_edu</th>\n",
       "      <th>min_credit_rev</th>\n",
       "      <th>max_credit_act</th>\n",
       "      <th>max_credit_act_rev</th>\n",
       "      <th>total_credit_1_miss</th>\n",
       "      <th>...</th>\n",
       "      <th>lines_deli</th>\n",
       "      <th>stress_index</th>\n",
       "      <th>lines_high_risk</th>\n",
       "      <th>max_due_ratio</th>\n",
       "      <th>auto_2_miss</th>\n",
       "      <th>card_type</th>\n",
       "      <th>location_id</th>\n",
       "      <th>default_ind</th>\n",
       "      <th>magic_uid</th>\n",
       "      <th>loc_woe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>236289.000000</td>\n",
       "      <td>1681.000000</td>\n",
       "      <td>1.281100</td>\n",
       "      <td>3.076</td>\n",
       "      <td>1.695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>5064.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>2775.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3247.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "      <td>-0.633713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>354149.000000</td>\n",
       "      <td>1889.000000</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>527.00000</td>\n",
       "      <td>17722.000000</td>\n",
       "      <td>17722.000000</td>\n",
       "      <td>21881.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.424760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69</td>\n",
       "      <td>-0.992980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>272397.497773</td>\n",
       "      <td>1718.442313</td>\n",
       "      <td>1.729757</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.33653</td>\n",
       "      <td>2423.463499</td>\n",
       "      <td>2423.463499</td>\n",
       "      <td>23706.328874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>20.810437</td>\n",
       "      <td>0.155529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0.908190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>244594.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.790</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1444.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>-1.016777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>269409.000000</td>\n",
       "      <td>1883.000000</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7730.00000</td>\n",
       "      <td>19820.000000</td>\n",
       "      <td>7730.000000</td>\n",
       "      <td>44605.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.971495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   application_key  credit_score  risk_score  sev_def_any  sev_def_auto  \\\n",
       "0    236289.000000   1681.000000    1.281100        3.076         1.695   \n",
       "1    354149.000000   1889.000000    0.503900        0.000         0.000   \n",
       "2    272397.497773   1718.442313    1.729757        0.000         0.000   \n",
       "3    244594.000000   1599.000000    0.000000       13.790         0.000   \n",
       "4    269409.000000   1883.000000    0.439200        0.000         0.000   \n",
       "\n",
       "   sev_def_edu  min_credit_rev  max_credit_act  max_credit_act_rev  \\\n",
       "0          0.0        10.00000     5064.000000         2007.000000   \n",
       "1          0.0       527.00000    17722.000000        17722.000000   \n",
       "2          0.0        20.33653     2423.463499         2423.463499   \n",
       "3          0.0         0.00000        0.000000            0.000000   \n",
       "4          0.0      7730.00000    19820.000000         7730.000000   \n",
       "\n",
       "   total_credit_1_miss  ...  lines_deli  stress_index  lines_high_risk  \\\n",
       "0          2775.000000  ...         0.0       0.54545         1.000000   \n",
       "1         21881.000000  ...         0.0       0.00000        16.000000   \n",
       "2         23706.328874  ...         0.0       0.00000        20.810437   \n",
       "3          1444.000000  ...         0.0       0.95652         0.000000   \n",
       "4         44605.000000  ...         0.0       0.00000         3.000000   \n",
       "\n",
       "   max_due_ratio  auto_2_miss  card_type  location_id  default_ind  magic_uid  \\\n",
       "0       0.948490          0.0        1.0       3247.0          0.0         61   \n",
       "1       0.424760          0.0        0.0        111.0          0.0         69   \n",
       "2       0.155529          0.0        0.0        712.0          1.0         71   \n",
       "3       1.000000          0.0        0.0          8.0          0.0         60   \n",
       "4       1.000000          0.0        0.0         22.0          0.0          2   \n",
       "\n",
       "    loc_woe  \n",
       "0 -0.633713  \n",
       "1 -0.992980  \n",
       "2  0.908190  \n",
       "3 -1.016777  \n",
       "4 -0.971495  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mf_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate numeric columns and perform qcuts\n",
    "\n",
    "num_cols = train_mf_res.drop(['application_key', 'card_type', 'location_id', 'default_ind'], axis=1).columns.tolist()\n",
    "train_bin = train_mf_res.drop(num_cols, axis=1)\n",
    "\n",
    "for col in num_cols:\n",
    "    enc_col = pd.cut(train_mf_res[col], 10, labels=[str(i+1) for i in range(10)], duplicates='drop')\n",
    "    train_bin[str(col)] = enc_col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type conversion for some columns\n",
    "\n",
    "train_bin['card_type'] = train_bin.card_type.apply(np.round).astype('int')\n",
    "train_bin['location_id'] = train_bin.location_id.astype('int')\n",
    "train_bin['default_ind'] = train_bin.default_ind.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to appropriate type\n",
    "\n",
    "cat_cols = train_bin.drop('application_key', axis=1).columns.tolist() \n",
    "train_bin[cat_cols] = train_bin[cat_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117534 entries, 0 to 117533\n",
      "Data columns (total 43 columns):\n",
      "application_key          117534 non-null float64\n",
      "card_type                117534 non-null category\n",
      "location_id              117534 non-null category\n",
      "default_ind              117534 non-null category\n",
      "credit_score             117534 non-null category\n",
      "risk_score               117534 non-null category\n",
      "sev_def_any              117534 non-null category\n",
      "sev_def_auto             117534 non-null category\n",
      "sev_def_edu              117534 non-null category\n",
      "min_credit_rev           117534 non-null category\n",
      "max_credit_act           117534 non-null category\n",
      "max_credit_act_rev       117534 non-null category\n",
      "total_credit_1_miss      117534 non-null category\n",
      "total_credit             117534 non-null category\n",
      "total_due                117534 non-null category\n",
      "annual_pay               117534 non-null category\n",
      "annual_income            117534 non-null category\n",
      "fc_cards_act_rev         117534 non-null category\n",
      "fc_cards_act             117534 non-null category\n",
      "fc_lines_act             117534 non-null category\n",
      "pc_cards_act             117534 non-null category\n",
      "pc_lines_act             117534 non-null category\n",
      "loan_util_act_rev        117534 non-null category\n",
      "line_util_past2          117534 non-null category\n",
      "line_util_1_miss         117534 non-null category\n",
      "tenure_act_rev           117534 non-null category\n",
      "tenure_oldest_act        117534 non-null category\n",
      "tenure_oldest_act_rev    117534 non-null category\n",
      "last_miss_time           117534 non-null category\n",
      "tenure_oldest_line       117534 non-null category\n",
      "total_tenure_act         117534 non-null category\n",
      "residence_time           117534 non-null category\n",
      "lines_act_1_miss         117534 non-null category\n",
      "lines_act                117534 non-null category\n",
      "cards_act_t2             117534 non-null category\n",
      "lines_act_2yrs           117534 non-null category\n",
      "lines_deli               117534 non-null category\n",
      "stress_index             117534 non-null category\n",
      "lines_high_risk          117534 non-null category\n",
      "max_due_ratio            117534 non-null category\n",
      "auto_2_miss              117534 non-null category\n",
      "magic_uid                117534 non-null category\n",
      "loc_woe                  117534 non-null category\n",
      "dtypes: category(42), float64(1)\n",
      "memory usage: 5.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_bin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this df to drive\n",
    "\n",
    "train_bin.to_csv(\".././mod_data/train_bin.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE after transforming categorical columns\n",
    "Since SMOTE converts our categorical columns to continuous if provided in their raw form, we will first convert our categorical columns to continuous by performing entity embeddings and then apply SMOTE. This way our columns will always remain continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load missing forest imputed data and split into train and test\n",
    "\n",
    "train_mf = pd.read_csv(\".././mod_data/train_mf.csv\")\n",
    "col_names = train_mf.columns.tolist()\n",
    "\n",
    "train_final = train_mf.values[:int(0.8*len(train_mf))]\n",
    "test_final = train_mf.values[int(0.8*len(train_mf)):]\n",
    "\n",
    "train_final = pd.DataFrame(train_final, columns=col_names).drop('application_key', axis=1)\n",
    "test_final = pd.DataFrame(test_final, columns=col_names).drop('application_key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating embeddings\n",
      "Epoch 1/1\n",
      "65972/65972 [==============================] - 1s 16us/step - loss: 0.6057 - binary_accuracy: 0.7112\n",
      "[INFO] Generating embeddings\n",
      "Epoch 1/1\n",
      "65972/65972 [==============================] - 1s 15us/step - loss: 0.6240 - binary_accuracy: 0.7144\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings\n",
    "\n",
    "loc_embed_df_train, loc_embed_df_test = create_embedding(train_final, test_final, 'location_id')\n",
    "card_embed_df_train, card_embed_df_test = create_embedding(train_final, test_final, 'card_type')\n",
    "\n",
    "# Scale numerical columns\n",
    "\n",
    "non_scale_cols = ['card_type', 'location_id', 'default_ind']\n",
    "X_df_train, X_df_test = scale_df(train_final, test_final, non_scale_cols, replace_cols='card_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace categorical columns by embeddings \n",
    "\n",
    "train_def_vals = train_final.default_ind.values\n",
    "train_final = pd.concat([X_df_train, loc_embed_df_train, card_embed_df_train], axis=1)\n",
    "train_final['default_ind'] = train_def_vals\n",
    "\n",
    "test_def_vals = test_final.default_ind.values\n",
    "test_final = pd.concat([X_df_test, loc_embed_df_test, card_embed_df_test], axis=1)\n",
    "test_final['default_ind'] = test_def_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increase in dataset size: 42.88% (28290 rows added)\n"
     ]
    }
   ],
   "source": [
    "# Perform SMOTE on training dataset\n",
    "\n",
    "cols = train_final.columns.tolist()\n",
    "y = train_final.default_ind.values\n",
    "X = train_final.drop(['default_ind'], axis=1).values\n",
    "\n",
    "# SMOTE resampling\n",
    "sm = smote(sampling_strategy='minority', random_state=123)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "pc_increase = (X_res.shape[0] - X.shape[0])*100/X.shape[0]\n",
    "print(\"Increase in dataset size: %.2f%% (%d rows added)\" % (pc_increase, X_res.shape[0] - X.shape[0]))\n",
    "\n",
    "# Reform dataframe\n",
    "res_df_vals = np.hstack((X_res, y_res.reshape((-1, 1))))\n",
    "res_df = pd.DataFrame(res_df_vals, columns=cols)\n",
    "\n",
    "train_final = res_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files to disk\n",
    "\n",
    "train_final.to_csv(\".././mod_data/train_final.csv\", index=False)\n",
    "test_final.to_csv(\".././mod_data/test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random over and under sampling\n",
    "Since SMOTE is causing issues related to data types and overall gives no benefit on test accuracy, we will try random over-sampling/under-sampling to see if that helps. Here, data types will remain consistent, so we know that our model is ready for the test data in it's raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import missForest imputed dataset\n",
    "\n",
    "train_mf = pd.read_csv(\".././mod_data/train_mf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_mf into permanent train and test\n",
    "\n",
    "col_names = train_mf.columns.tolist()\n",
    "\n",
    "train_rs = pd.DataFrame(train_mf.values[:int(0.8*len(train_mf))], columns=col_names)\n",
    "test_rs = pd.DataFrame(train_mf.values[int(0.8*len(train_mf)):], columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversample and undersample in train_rs\n",
    "\n",
    "count_class_0, count_class_1 = train_rs.default_ind.value_counts()\n",
    "train_rs_class_0 = train_rs[train_rs.default_ind == 0]\n",
    "train_rs_class_1 = train_rs[train_rs.default_ind == 1]\n",
    "\n",
    "class_1_over = train_rs_class_1.sample(count_class_0, replace=True)\n",
    "train_rs_over = pd.concat([train_rs_class_0, class_1_over], axis=0)\n",
    "\n",
    "class_0_under = train_rs_class_0.sample(count_class_1, replace=True)\n",
    "train_rs_under = pd.concat([class_0_under, train_rs_class_1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop application key columns and save dataFrames \n",
    "\n",
    "train_rs_over = train_rs_over.drop('application_key', axis=1)\n",
    "train_rs_under = train_rs_under.drop('application_key', axis=1)\n",
    "test_rs = test_rs.drop('application_key', axis=1)\n",
    "\n",
    "train_rs_over.to_csv(\"/home/nishant/Desktop/IDA Project/rs_data/train_os.csv\", index=False)\n",
    "train_rs_under.to_csv(\"/home/nishant/Desktop/IDA Project/rs_data/train_us.csv\", index=False)\n",
    "test_rs.to_csv(\"/home/nishant/Desktop/IDA Project/rs_data/test_rs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rs_over.to_csv(\"/home/nishant/Desktop/IDA Project/rs_data/train_os.tsv\", sep='\\t', index=False)\n",
    "train_rs_under.to_csv(\"/home/nishant/Desktop/IDA Project/rs_data/train_us.tsv\", sep='\\t', index=False)\n",
    "test_rs.to_csv(\"/home/nishant/Desktop/IDA Project/rs_data/test_rs.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard dataset cleaning\n",
    "Final operations for the leaderboard dataset have been performed here. Operations have been decided based on model performance on differently modified datasets in the Model Selection notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset final\n",
    "\n",
    "test_final = pd.read_csv(\".././data/testX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_key</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>sev_def_any</th>\n",
       "      <th>sev_def_auto</th>\n",
       "      <th>sev_def_edu</th>\n",
       "      <th>min_credit_rev</th>\n",
       "      <th>max_credit_act</th>\n",
       "      <th>max_credit_act_rev</th>\n",
       "      <th>total_credit_1_miss</th>\n",
       "      <th>...</th>\n",
       "      <th>lines_deli</th>\n",
       "      <th>line_util_edu</th>\n",
       "      <th>line_util_auto</th>\n",
       "      <th>stress_index</th>\n",
       "      <th>lines_high_risk</th>\n",
       "      <th>max_due_ratio</th>\n",
       "      <th>mort_2_miss</th>\n",
       "      <th>auto_2_miss</th>\n",
       "      <th>card_type</th>\n",
       "      <th>location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578069</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.6174</td>\n",
       "      <td>8.623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>10729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.54545</td>\n",
       "      <td>2</td>\n",
       "      <td>0.91837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>3247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578070</td>\n",
       "      <td>1795</td>\n",
       "      <td>0.2051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1685</td>\n",
       "      <td>12711</td>\n",
       "      <td>8913</td>\n",
       "      <td>80519</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.17241</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578071</td>\n",
       "      <td>1742</td>\n",
       "      <td>0.5082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>8954</td>\n",
       "      <td>8954</td>\n",
       "      <td>1189</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.64706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578072</td>\n",
       "      <td>1685</td>\n",
       "      <td>0.2595</td>\n",
       "      <td>25.409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>3354</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578073</td>\n",
       "      <td>1666</td>\n",
       "      <td>1.2678</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>missing</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>101.61</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99617</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   application_key credit_score  risk_score  sev_def_any  sev_def_auto  \\\n",
       "0           578069         1719      0.6174        8.623           0.0   \n",
       "1           578070         1795      0.2051        0.000           0.0   \n",
       "2           578071         1742      0.5082        0.000           0.0   \n",
       "3           578072         1685      0.2595       25.409           0.0   \n",
       "4           578073         1666      1.2678        0.000           0.0   \n",
       "\n",
       "   sev_def_edu min_credit_rev max_credit_act max_credit_act_rev  \\\n",
       "0          0.0            258            258                258   \n",
       "1          0.0           1685          12711               8913   \n",
       "2          0.0           1185           8954               8954   \n",
       "3          0.0        missing           3354            missing   \n",
       "4          0.0            570            570                570   \n",
       "\n",
       "  total_credit_1_miss  ... lines_deli line_util_edu line_util_auto  \\\n",
       "0               10729  ...          0       missing        missing   \n",
       "1               80519  ...          0       missing        missing   \n",
       "2                1189  ...          0       missing        missing   \n",
       "3             missing  ...          0       missing        missing   \n",
       "4             missing  ...          0       missing         101.61   \n",
       "\n",
       "  stress_index  lines_high_risk max_due_ratio mort_2_miss auto_2_miss  \\\n",
       "0      0.54545                2       0.91837           0           0   \n",
       "1      0.17241                4       0.94563           0           0   \n",
       "2      0.64706                1       0.97054           0           0   \n",
       "3      0.85714                1       1.00000          na           0   \n",
       "4      missing                0       0.99617          na           0   \n",
       "\n",
       "  card_type location_id  \n",
       "0         C        3247  \n",
       "1         C          18  \n",
       "2         C          11  \n",
       "3         C          32  \n",
       "4         L          89  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_tags = ['missing', 'na', 'N/A']\n",
    "\n",
    "for col in test_final.columns:\n",
    "    for i in range(len(test_final)):\n",
    "        if test_final.at[i, col] in miss_tags:\n",
    "            test_final.at[i, col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_key</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>sev_def_any</th>\n",
       "      <th>sev_def_auto</th>\n",
       "      <th>sev_def_edu</th>\n",
       "      <th>min_credit_rev</th>\n",
       "      <th>max_credit_act</th>\n",
       "      <th>max_credit_act_rev</th>\n",
       "      <th>total_credit_1_miss</th>\n",
       "      <th>...</th>\n",
       "      <th>lines_deli</th>\n",
       "      <th>line_util_edu</th>\n",
       "      <th>line_util_auto</th>\n",
       "      <th>stress_index</th>\n",
       "      <th>lines_high_risk</th>\n",
       "      <th>max_due_ratio</th>\n",
       "      <th>mort_2_miss</th>\n",
       "      <th>auto_2_miss</th>\n",
       "      <th>card_type</th>\n",
       "      <th>location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578069</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.6174</td>\n",
       "      <td>8.623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>10729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54545</td>\n",
       "      <td>2</td>\n",
       "      <td>0.91837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>3247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578070</td>\n",
       "      <td>1795</td>\n",
       "      <td>0.2051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1685</td>\n",
       "      <td>12711</td>\n",
       "      <td>8913</td>\n",
       "      <td>80519</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17241</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578071</td>\n",
       "      <td>1742</td>\n",
       "      <td>0.5082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>8954</td>\n",
       "      <td>8954</td>\n",
       "      <td>1189</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97054</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578072</td>\n",
       "      <td>1685</td>\n",
       "      <td>0.2595</td>\n",
       "      <td>25.409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578073</td>\n",
       "      <td>1666</td>\n",
       "      <td>1.2678</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   application_key credit_score  risk_score  sev_def_any  sev_def_auto  \\\n",
       "0           578069         1719      0.6174        8.623           0.0   \n",
       "1           578070         1795      0.2051        0.000           0.0   \n",
       "2           578071         1742      0.5082        0.000           0.0   \n",
       "3           578072         1685      0.2595       25.409           0.0   \n",
       "4           578073         1666      1.2678        0.000           0.0   \n",
       "\n",
       "   sev_def_edu min_credit_rev max_credit_act max_credit_act_rev  \\\n",
       "0          0.0            258            258                258   \n",
       "1          0.0           1685          12711               8913   \n",
       "2          0.0           1185           8954               8954   \n",
       "3          0.0            NaN           3354                NaN   \n",
       "4          0.0            570            570                570   \n",
       "\n",
       "  total_credit_1_miss  ... lines_deli line_util_edu line_util_auto  \\\n",
       "0               10729  ...          0           NaN            NaN   \n",
       "1               80519  ...          0           NaN            NaN   \n",
       "2                1189  ...          0           NaN            NaN   \n",
       "3                 NaN  ...          0           NaN            NaN   \n",
       "4                 NaN  ...          0           NaN         101.61   \n",
       "\n",
       "  stress_index  lines_high_risk max_due_ratio mort_2_miss auto_2_miss  \\\n",
       "0      0.54545                2       0.91837           0           0   \n",
       "1      0.17241                4       0.94563           0           0   \n",
       "2      0.64706                1       0.97054           0           0   \n",
       "3      0.85714                1       1.00000         NaN           0   \n",
       "4          NaN                0       0.99617         NaN           0   \n",
       "\n",
       "  card_type location_id  \n",
       "0         C        3247  \n",
       "1         C          18  \n",
       "2         C          11  \n",
       "3         C          32  \n",
       "4         L          89  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final.to_csv(\".././mod_data/test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode card type\n",
    "card_map = {'C': 0.0, 'L': 1.0}\n",
    "test_final['card_type'] = test_final['card_type'].map(card_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test_final.columns\n",
    "test_final[cols]=test_final[cols].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod = pd.read_csv(\".././mod_data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\".././data/train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
